{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import avg, round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the Spark cluster, we must create a SparkSession object with the following params:\n",
    "\n",
    "* **appName**: application name displayed at the Spark Master Web UI;\n",
    "* **master**: Spark Master URL, same used by Spark Workers;\n",
    "* **spark.executor.memory**: must be less than or equals to docker compose SPARK_WORKER_MEMORY config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 08:44:04,461 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('spark-notebook-1').master('spark://spark-master:7077').config('spark.executor.memmory', '2G').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from csv ./data/covid.csv\n"
     ]
    }
   ],
   "source": [
    "# The data from covid.csv will be loaded from a local file path\n",
    "for path, subdirs, files in os.walk('./data/'):\n",
    "    for name in files:\n",
    "        if \"covid\" in name:\n",
    "            csvName = name\n",
    "            csvPath = os.path.join(path, name)\n",
    "            print(\"Loading data from csv {}\".format(csvPath))\n",
    "            covidDfPandas = pandas.read_csv(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of data in the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"dateRep\", StringType(), True),\n",
    "    StructField(\"day\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"cases\", IntegerType(), True),\n",
    "    StructField(\"deaths\", IntegerType(), True),\n",
    "    StructField(\"countriesAndTerritories\", StringType(), True),\n",
    "    StructField(\"geoId\", StringType(), True),\n",
    "    StructField(\"countryterritoryCode\", StringType(), True),\n",
    "    StructField(\"popData2019\", FloatType(), True),\n",
    "    StructField(\"continentExp\", StringType(), True),\n",
    "    StructField(\"Cumulative_number_for_14_days_of_COVID-19_cases_per_100000\", DoubleType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PySpark DataFrame from Pandas\n",
    "covidDfSpark = spark.createDataFrame(covidDfPandas, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of cases with Covid in October of 2020 in Belgium:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 13:00:32,267 WARN scheduler.TaskSetManager: Stage 0 contains a task of very large size (1650 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(cases)|\n",
      "+----------+\n",
      "|    320023|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The sum of cases with Covid in October of 2020 in Belgium.\n",
    "\n",
    "cases_Greece = covidDfSpark.filter((covidDfSpark['month'] == 10) & (covidDfSpark['year'] == 2020) & (covidDfSpark['countryterritoryCode'] == 'BEL'))\n",
    "\n",
    "print('Sum of cases with Covid in October of 2020 in Belgium:')\n",
    "cases_Greece.agg({'cases':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of cases and deaths for each country:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 21:40:06,991 WARN scheduler.TaskSetManager: Stage 2 contains a task of very large size (1650 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+\n",
      "|countryterritoryCode|sum(cases)|sum(deaths)|\n",
      "+--------------------+----------+-----------+\n",
      "|                 HTI|      9565|        234|\n",
      "|                 PSE|    124657|       1079|\n",
      "|                 LVA|     25675|        349|\n",
      "|                 POL|   1135676|      22864|\n",
      "|                 BRB|       292|          7|\n",
      "|                 ZMB|     18274|        367|\n",
      "|                 JAM|     11710|        273|\n",
      "|                 BRA|   6901952|     181402|\n",
      "|                 ARM|    148682|       2503|\n",
      "|                 MOZ|     16954|        142|\n",
      "+--------------------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The sum of cases and deaths for each country.\n",
    "\n",
    "group_continents = covidDfSpark.groupBy('countryterritoryCode').agg({'cases':'sum', 'deaths':'sum'})\n",
    "\n",
    "print('Sum of cases and deaths for each country:')\n",
    "group_continents.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of cases and deaths in the countries of Asia:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 21:40:08,140 WARN scheduler.TaskSetManager: Stage 8 contains a task of very large size (1650 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 9:===================================>                   (128 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+\n",
      "|          country|Average of cases|Average of deaths|\n",
      "+-----------------+----------------+-----------------+\n",
      "|      Afghanistan|          144.92|              5.8|\n",
      "|          Bahrain|          255.42|              1.0|\n",
      "|       Bangladesh|         1783.76|            25.64|\n",
      "|           Bhutan|            1.59|              0.0|\n",
      "|Brunei_Darussalam|            0.54|             0.01|\n",
      "|         Cambodia|            1.05|              0.0|\n",
      "|            China|          262.92|            13.54|\n",
      "|            India|         28321.2|           410.76|\n",
      "|        Indonesia|         1801.22|            54.87|\n",
      "|             Iran|         3166.48|           149.13|\n",
      "+-----------------+----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The average number of cases and deaths in the countries of Asia.\n",
    "\n",
    "cases_Asia = covidDfSpark.filter(covidDfSpark['continentExp'] == 'Asia').withColumnRenamed('countriesAndTerritories', 'country')\n",
    "\n",
    "result = cases_Asia.groupBy('country').agg(\n",
    "    round(avg('cases'), 2).alias('Average of cases'),\n",
    "    round(avg('deaths'), 2).alias('Average of deaths') \n",
    ").orderBy('country')\n",
    "\n",
    "print('Average number of cases and deaths in the countries of Asia:')\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of deaths of Covid in Europe by date:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 21:40:09,981 WARN scheduler.TaskSetManager: Stage 10 contains a task of very large size (1650 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 11:==================================================>   (186 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|   dateRep|sum(deaths)|\n",
      "+----------+-----------+\n",
      "|25/11/2020|       6391|\n",
      "|02/12/2020|       6195|\n",
      "|09/12/2020|       6074|\n",
      "|28/11/2020|       6001|\n",
      "|21/11/2020|       5909|\n",
      "|18/11/2020|       5829|\n",
      "|05/12/2020|       5783|\n",
      "|03/12/2020|       5728|\n",
      "|26/11/2020|       5719|\n",
      "|04/12/2020|       5716|\n",
      "+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The sum of deaths of Covid in Europe by date.\n",
    "\n",
    "cases_Europe = covidDfSpark.filter(covidDfSpark['continentExp'] == 'Europe')\n",
    "\n",
    "print('Sum of deaths of Covid in Europe by date:')\n",
    "cases_Europe.groupBy('dateRep').agg({'deaths':'sum'}).sort('sum(deaths)', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochNow = int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 21:40:25,566 WARN scheduler.TaskSetManager: Stage 12 contains a task of very large size (1650 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 12:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covid Dataframe stored in Hadoop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write Dataframe into HDFS as a parquet file\n",
    "\n",
    "covidDfSpark.write.parquet(\"hdfs://namenode:8020/covid/{}_{}.parquet\".format(csvName,epochNow))\n",
    "print(\"Covid Dataframe stored in Hadoop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covid Dataframe read from Hadoop : \n",
      "+----------+---+-----+----+-----+------+-----------------------+-----+--------------------+-----------+------------+----------------------------------------------------------+\n",
      "|   dateRep|day|month|year|cases|deaths|countriesAndTerritories|geoId|countryterritoryCode|popData2019|continentExp|Cumulative_number_for_14_days_of_COVID-19_cases_per_100000|\n",
      "+----------+---+-----+----+-----+------+-----------------------+-----+--------------------+-----------+------------+----------------------------------------------------------+\n",
      "|08/09/2020|  8|    9|2020|   80|     1|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               36.33682539|\n",
      "|07/09/2020|  7|    9|2020|  199|     1|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               35.90559174|\n",
      "|06/09/2020|  6|    9|2020|  896|     8|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               36.22901698|\n",
      "|05/09/2020|  5|    9|2020|  104|     0|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                                36.5740039|\n",
      "|04/09/2020|  4|    9|2020|  570|     4|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               40.90790208|\n",
      "|03/09/2020|  3|    9|2020|  659|     6|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               43.11258411|\n",
      "|02/09/2020|  2|    9|2020|   99|    68|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               45.11782058|\n",
      "|01/09/2020|  1|    9|2020|  923|    34|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               51.87201762|\n",
      "|31/08/2020| 31|    8|2020|    0|     0|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               48.69705987|\n",
      "|30/08/2020| 30|    8|2020|  633|     0|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               51.27368093|\n",
      "|29/08/2020| 29|    8|2020| 1422|     0|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               55.30032513|\n",
      "|28/08/2020| 28|    8|2020|  954|     0|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               54.46480993|\n",
      "|27/08/2020| 27|    8|2020|  202|     0|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               56.92284173|\n",
      "|26/08/2020| 26|    8|2020|    0|     0|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               63.86031307|\n",
      "|25/08/2020| 25|    8|2020|    0|   108|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               75.25566226|\n",
      "|24/08/2020| 24|    8|2020|  259|     2|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               79.45479992|\n",
      "|23/08/2020| 23|    8|2020|  960|     8|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               82.91005954|\n",
      "|22/08/2020| 22|    8|2020|  908|     5|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               87.88541777|\n",
      "|21/08/2020| 21|    8|2020|  979|     6|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                               93.82027087|\n",
      "|20/08/2020| 20|    8|2020| 1031|     6|             Kazakhstan|   KZ|                 KAZ|1.8551428E7|        Asia|                                              101.50162025|\n",
      "+----------+---+-----+----+-----+------+-----------------------+-----+--------------------+-----------+------------+----------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from HDFS to confirm it was successfully stored\n",
    "\n",
    "df_load = spark.read.parquet(\"hdfs://namenode:8020/covid/{}_{}.parquet\".format(csvName,epochNow))\n",
    "print(\"Covid Dataframe read from Hadoop : \")\n",
    "df_load.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
