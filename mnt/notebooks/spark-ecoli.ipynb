{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e3c7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1027793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b980766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new SparkSession\n",
    "spark2 = SparkSession.builder.appName('spark-notebook-2').master('spark://spark-master:7077').getOrCreate()\n",
    "\n",
    "# Create the schema\n",
    "schema = StructType([\n",
    "    StructField(\"0\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5b93ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from text file: ./data/ecoli.txt\n"
     ]
    }
   ],
   "source": [
    "# The data from covid.csv will be loaded from a local file path\n",
    "for path, subdirs, files in os.walk('./data/'):\n",
    "    for name in files:\n",
    "        if \"ecoli\" in name and name.endswith(\".txt\"):\n",
    "            txtPath = os.path.join(path, name)\n",
    "            print(\"Loading data from text file: {}\".format(txtPath))\n",
    "            # Load the ecoli.txt file\n",
    "            lines_txt = pandas.read_csv(txtPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "390c7899",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_sp = spark2.createDataFrame(lines_txt, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c23a1ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 09:49:12,021 WARN scheduler.TaskSetManager: Stage 0 contains a task of very large size (2405 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   0|\n",
      "+--------------------+\n",
      "|TTCTGAACTGGTTACCT...|\n",
      "|TATAGGCATAGCGCACA...|\n",
      "|ATTACCACCACCATCAC...|\n",
      "|CCCGCACCTGACAGTGC...|\n",
      "|GTTCGGCGGTACATCAG...|\n",
      "|AGGCAGGGGCAGGTGGC...|\n",
      "|AAAAAACCATTAGCGGC...|\n",
      "|GACGGGACTCGCCGCCG...|\n",
      "|GCCCAAATAAAACATGT...|\n",
      "|TGATTTGCCGTGGCGAG...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Preview the structure\n",
    "lines_sp.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca557824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to an RDD\n",
    "lines_rdd = lines_sp.rdd\n",
    "\n",
    "# Define the lengths for patterns (3 and 4)\n",
    "pattern_lengths = [3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c503c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_rdd = lines_rdd.map(lambda row: row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1dfc00c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "764e7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_grams(line, length):\n",
    "    ngrams = []\n",
    "    for i in range(len(line) - length + 1):\n",
    "        ngrams.append(line[i : i + length])\n",
    "    return ngrams\n",
    "\n",
    "pattern_lengths = [3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "11ffe845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD of n-grams for each pattern length\n",
    "ngrams_rdds = []\n",
    "for length in pattern_lengths:\n",
    "    ngrams_rdd = lines_rdd.flatMap(lambda line: line_to_grams(line, length))\n",
    "    ngrams_rdds.append(ngrams_rdd)\n",
    "\n",
    "# Calculate the counts of n-grams\n",
    "count_rdds = []\n",
    "for ngrams_rdd, length in zip(ngrams_rdds, pattern_lengths):\n",
    "    count_rdd = ngrams_rdd.map(lambda ngram: (ngram, 1)).reduceByKey(lambda a, b: a + b).map(lambda x: (x[1], x[0]))\n",
    "    count_rdds.append(count_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f74c5b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PythonRDD[20] at RDD at PythonRDD.scala:53,\n",
       " PythonRDD[21] at RDD at PythonRDD.scala:53]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_rdds # The two rdds created have the data for each pattern length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e60ed7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rdd = count_rdds[0].union(count_rdds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f2693048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 09:49:17,731 WARN scheduler.TaskSetManager: Stage 1 contains a task of very large size (2405 KiB). The maximum recommended task size is 1000 KiB.\n",
      "2023-09-13 09:49:19,898 WARN scheduler.TaskSetManager: Stage 2 contains a task of very large size (2405 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dna_patterns = merged_rdd.sortByKey(ascending=False).map(lambda x: f\"{x[1]} {x[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a47b6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CGC 112398',\n",
       " 'GCG 111418',\n",
       " 'TTT 106672',\n",
       " 'AAA 105850',\n",
       " 'CAG 101927',\n",
       " 'CTG 99892',\n",
       " 'GCA 93338',\n",
       " 'TGC 92456',\n",
       " 'GCC 90317',\n",
       " 'GGC 89538']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_patterns.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9102bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each string into two columns (ngram and count) using space as the delimiter\n",
    "split_rdd = dna_patterns.map(lambda line: line.split())\n",
    "\n",
    "# Create a DataFrame from the RDD with column names\n",
    "columns = [\"pattern\", \"count\"]\n",
    "df = spark2.createDataFrame(split_rdd, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c6eb06ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|pattern| count|\n",
      "+-------+------+\n",
      "|    CGC|112398|\n",
      "|    GCG|111418|\n",
      "|    TTT|106672|\n",
      "|    AAA|105850|\n",
      "|    CAG|101927|\n",
      "|    CTG| 99892|\n",
      "|    GCA| 93338|\n",
      "|    TGC| 92456|\n",
      "|    GCC| 90317|\n",
      "|    GGC| 89538|\n",
      "+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ea2f1b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6ec87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
